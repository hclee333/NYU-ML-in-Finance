# ## Part 2: Explore the implications of calibrated model parameters for default probabilities of stocks in your portfolio. Present your conclusions and observations. (Max 10 point).
# 
# In this part, you will experiment with other signals. Propose a signal and explain why it is interesting to 
# include this signal in the portfolio analysis. Then add your favorite signal or signals to the previous benchmarck signals (or alternatively you can replace them), and repeat the analysis of model calibration. State your conclusions.
# 

# In[35]:


# Put the rest of your code and analysis for Part 2 here.
from sklearn.preprocessing import MinMaxScaler

# Converting data to a log space and scale to distributions
df_cap_log = np.log(df_cap)
short_rolling_log = df_cap_log.rolling(window=window_1).mean()
long_rolling_log = df_cap_log.rolling(window=window_2).mean()

df_cap_scaler = MinMaxScaler([0.1, 1]).fit(df_cap_log)

df_cap_norm = pd.DataFrame(data=df_cap_scaler.transform(df_cap_log),
                           columns=df_cap_log.columns,
                           index=df_cap_log.index)

# Rescaling the signals
long_rolling_norm = pd.DataFrame(data=df_cap_scaler.transform(long_rolling_log.dropna()),
                                 columns=long_rolling_log.columns,
                                 index=long_rolling_log.dropna().index)

short_rolling_norm = pd.DataFrame(data=df_cap_scaler.transform(short_rolling_log.dropna()),
                                  columns=short_rolling_log.columns,
                                  index=short_rolling_log.dropna().index)

# Signals ar ethe percentage changes
signal_1 = short_rolling_norm.pct_change(periods=1).shift(-1).dropna().copy()
signal_2 = long_rolling_norm.pct_change(periods=1).shift(-1).dropna().copy()

# Consider the same times
market = df_cap_norm[df_cap_norm.index.isin(signal_1.index) & df_cap_norm.index.isin(signal_2.index)]
signal_1 = signal_1[signal_1.index.isin(market.index)]
signal_2 = signal_2[signal_2.index.isin(market.index)]


# In[36]:


# Inverse Reinforcement Learning code 
def fit_IRL_model(num_timesteps, signal_1, signal_2, epochs=1000, tol=1, learning_rate=1.5e-3):

    start = time.time()
    assert not np.any(np.isnan(market))
    assert not np.any(np.isnan(signal_1))
    assert not np.any(np.isnan(signal_2))

    t = market.shape[0]
    n = market.shape[1]

    # Set up the graph
    tf.reset_default_graph()
    x  = tf.placeholder(shape = (None, n), dtype = tf.float32, name = 'x' )  # market
    z1 = tf.placeholder(shape = (None, n), dtype = tf.float32, name = 'z1' ) # signal 1
    z2 = tf.placeholder(shape = (None, n), dtype = tf.float32, name = 'z2' ) # sigmal 2

    # Weights
    w1_init = tf.random_normal([n], mean = 0.5, stddev = 0.1)
    w2_init = 1 - w1_init
    w1      = tf.get_variable('w1', initializer = w1_init)
    w2      = tf.get_variable('w2', initializer = w2_init)
    W1      = w1 * tf.ones(n)
    W2      = w2 * tf.ones(n)

    # Parameters
    sigma = tf.get_variable('sigma', initializer = tf.random_uniform([n], minval=0.0, maxval=0.1))
    kappa = tf.get_variable('kappa', initializer = tf.random_uniform([n], minval=0.0, maxval=1.0))
    g     = tf.get_variable('g',     initializer = tf.random_uniform([n], minval=0.0, maxval=1.0))
    theta = tf.get_variable('theta', initializer = tf.random_uniform([n], minval=0.0, maxval=1.0))
    mu    = tf.zeros([n])
    Sigma = sigma * tf.ones([n])
    Kappa = kappa * tf.ones([n])
    G     = g     * tf.ones([n])
    Theta = theta * tf.ones([n])
    signals1 = tf.multiply(W1, z1)
    signals2 = tf.multiply(W2, z2)
    scale    = tf.slice(x, [0,0], [1,-1])
    signals  = tf.multiply(scale, tf.cumprod(1 + tf.add(signals1, signals2)))

    # Non-negative constraints for the weights
    w1_abs      = w1.assign(tf.maximum(0., w1))
    w2_abs      = w2.assign(tf.maximum(0., w2))
    g_abs       = g.assign(tf.maximum(0., g))
    constraints = tf.group(w1_abs, w2_abs)
                
    # IRL expression
    term0 = tf.add(tf.subtract(Theta, 0.5 * tf.square(Sigma)), signals)
    term1 = tf.multiply(-term0, x)
    term2 = tf.multiply(Kappa, tf.exp(x))
    term3 = tf.multiply(0.5 * G, tf.exp(2 * x))
    v     = tf.add(tf.add(term1, term2), term3)
    V = tf.slice(v, [0,0], [tf.shape(v)[0]-1, -1])

    distribution = tf.contrib.distributions.MultivariateNormalDiag(loc=mu, scale_diag=Sigma)
    log_prob     = distribution.log_prob(V)
    reg_term     = tf.reduce_sum(tf.square(w1 + w2 - 1))
    neg_log_likelihood = 0.01 * reg_term - tf.reduce_sum(log_prob)
    
    # Configuration the optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)
    train_op  = optimizer.minimize(neg_log_likelihood)
        
    print('The optimizer has been configured!')
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        # run the first iteration and calculate initial loss
        res = []
        loss = []
        res.append(sess.run(neg_log_likelihood, feed_dict = {x: market, z1: signal_1, z2: signal_2}))

        # Iterate loss minimization in a loop
        e=1
        while True:
            # Solve the optimiser
            sess.run(train_op, feed_dict={x: market, z1: signal_1, z2: signal_2})
            sess.run(constraints) 

            # Update loss
            res.append(sess.run(neg_log_likelihood, feed_dict={x: market, z1: signal_1, z2: signal_2}))
            loss_ = np.abs(res[-1] - res[-2])
            loss.append(loss_)

            if (e % 200 == 0) or (e == 1):
                print('Epoch {:5}: Loss: {}'.format(e, loss_))

            if loss_ < tol:
                print('Converged after {} epochs. Loss: {}'.format(e, loss_))
                break

            if e >= epochs:
                print('Specified maximum epochs ({}) reached without convergence. Loss: {}'.format(epochs, loss_))
                break

            e += 1
                
        end = time.time()            
            
        # Save the coefficients
        coefficients = pd.DataFrame([], index = market.columns, columns = ['kappa', 'sigma', 'theta', 'g', 'w1', 'w2'] )
        coefficients['kappa']   = sess.run(kappa)
        coefficients['sigma']   = sess.run(sigma)
        coefficients['theta']   = sess.run(theta)
        coefficients['g']       = sess.run(g)
        coefficients['w1']      = sess.run(W1)
        coefficients['w2']      = sess.run(W2)

        est_ = sess.run(signals, feed_dict={x: market, z1: signal_1, z2: signal_2})
        estimation = pd.DataFrame(est_, index=market.index, columns=market.columns)
    
    return estimation
    print('All done. Elapsed time:', timedelta(seconds=end-start))


# In[40]:


import time, random
from datetime import timedelta


tf.logging.set_verbosity(tf.logging.ERROR)
estimation_scaled = fit_IRL_model(market, signal_1, signal_2, epochs=5000, tol=2)


# In[41]:


estimation = df_cap_scaler.inverse_transform(estimation_scaled)
df_est = pd.DataFrame(estimation, columns=estimation_scaled.columns, index=estimation_scaled.index)


# In[42]:



fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)
plt.suptitle('Market cap demeaned and stanadrdized for AAPL',size=20)

ax.plot(df_cap_log.index, df_cap_log.loc[:, 'AAPL'], label='Original')
ax.plot(df_est.index, df_est.loc[:, 'AAPL'], 
          label = 'Estimator')

ax.legend(loc='best')
ax.set_ylabel('log-ret')

plt.show()





