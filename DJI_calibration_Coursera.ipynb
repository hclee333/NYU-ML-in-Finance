
# coding: utf-8

# ## Econometric estimation of an IRL-based market portfolio model
# 
# Welcome to your final course project on RL in Finance. In this project you will: 
# 
# - Explore and estimate an IRL-based model of market returns that is based on IRL of a market-optimal portfolio 
# - Investigate the role and impact of choices of different signals on model estimation and trading strategies
# - Compare simple IRL-based and UL-based trading strategies
# 
# **Instructions for project structure and grading principles :**
# 
# - This is a project that will be graded based on a peer-to-peer review. The project consists of four parts. The maximum score for each part is 10, so that maximum score you can give your peers (and they can give you) is 40. The parts are as follows (more detailed instructions are in specific cells below):
# 
# - **Part 1**: Complete the model estimation for the DJI portfolio of 30 stocks, and simple signals such as simple moving averages constructed below (Max 10 point).
# 
# - **Part 2**: Propose other signals and investigate the dynamics for market caps obtained with alternative signals. Present your conclusions and observations. (Max 10 point).
# 
# - **Part 3**: Can you repeat your analysis for the S&P portfolio? You will have to build a data file, build signals, and repeat the model estimation process with your new dataset (Max 10 points).
# 
# - **Part 4** : Show me something else. This part is optional. Come up with your own idea of an interesting analysis. For example, you can build a strategy using an optimal market-implied policy estimated from this model, and compare it with PCA and absorption ratio strategies that we built in Course 2. (Max 10 points).
# 
# **Instructions for formatting your notebook and packages use can use **
# 
# - Use one or more cells of the notebook for each section of the project. Each section is marked by a header cell below. Insert your cells between them without changing the sequence. 
# 
# - Think of an optimal presentation of your results and conclusions. Think of how hard or easy it will be for your fellow students to follow your logic and presentation. When you are grading others, you can add or subtract point for the quality of presentation.
# 
# - You will be using Python 3 in this project. Using TensorFlow is encouraged but is not strictly necessary, you can use optimization algorithms available in scipy or scikit-learn packages. If you use any non-standard packages, you should state all neccessary additional imports (or instructions how to install any additional modules you use in a top cell of your notebook. If you create a new portfolio for parts 3 and 4 in the project, make your code for creating your dataset replicable as well, so that your grader can reproduce your code locally on his/her machine.   
# 
# - Try to write a clean code that can be followed by your peer reviewer. When you are the reviewer, you can add or subtract point for the quality of code. 
# 
# 
# **After completing this project you will:**
# - Get experience with building and estimation of your first IRL based model of market dynamics, and learn how this IRL approach extends the famous Black-Litterman model (see F. Black and R. Litterman, "Global Portfolio Optimization", Financial Analyst Journal, Sept-Oct. 1992, 28-43, and  D. Bertsimas, V. Gupta, and I.Ch. Paschalidis, "Inverse Optimization: A New Perspective on the Black-Litterman Model", Operations Research, Vol.60, No.6, pp. 1389-1403 (2012), I.Halperin and I. Feldshteyn "Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy", https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3174498.). 
# - Know how to enhance a market-optimal portfolio policy by using your private signals. 
# - Be able to implement trading strategies based on this method.
# 
# Let's get started!

# ## The IRL-based model of stock returns
# 
# In Week 4 lectures of our course we found that optimal investment policy in the problem of inverse portfolio optimization is a Gaussian policy
# 
# $$ \pi_{\theta}({\bf a}_t |{\bf y}_t ) =   \mathcal{N}\left({\bf a}_t | \bf{A}_0 + \bf{A}_1 {\bf y}_t, \Sigma_p \right) $$
# 
# Here $ {\bf y}_t $ is a vector of dollar position in the portfolio, and $ \bf{A}_0 $, $ \bf{A}_1 $ and $ \Sigma_p $ are parameters defining a Gaussian policy.   
# 
# We said in the lecture that such Gaussian policy is found for both cases of a single investor and a market portfolio. We also sketched a numerical scheme that can iteratively compute coefficients $ \bf{A}_0$, $ \bf{A}_1 $ and $ \Sigma_p $ using a combination of a RL algorithm called G-learning and a trajectory optimization algorithm.
# 
# In this project, you will explore implications and estimation of this IRL-based model for the most interesting case - the market portfolio. It turns out that for this case, the model can be estimated in an easier way using a conventional Maximum Likelihood approach. To this end, we will re-formulate the model for this particular case in three easy steps.
# 
# 
# Recall that for a vector of $ N $ stocks, we introduced a size $ 2 N $-action vector 
# $ {\bf a}_t = [{\bf u}_t^{(+)}, {\bf u}_t^{(-)}] $, so that an action $ {\bf u}_t $ was defined as a difference of two non-negative numbers 
# $ {\bf u}_t = {\bf u}_t^{(+)} -  {\bf u}_t^{(-)} = [{\bf 1}, - {\bf 1}] {\bf a}_t \equiv {\bf 1}_{-1}^{T} {\bf a}_t $.
# 
# Therefore, the joint distribution of $ {\bf a}_t = [{\bf u}_t^{(+)}, {\bf u}_t^{(-)} ] $ is given by our Gaussian policy
# $  \pi_{\theta}({\bf a}_t |{\bf y}_t ) $. This means that the distribution of 
# $ {\bf u}_t = {\bf u}_t^{(+)} -  {\bf u}_t^{(-)} $ is also Gaussian. Let us write it therefore as follows:
# 
# $$
# \pi_{\theta}({\bf u}_t |{\bf y}_t ) =   \mathcal{N}\left({\bf u}_t | \bf{U}_0 + \bf{U}_1 {\bf y}_t, \Sigma_u \right) 
# $$
# 
# Here $ \bf{U}_0 = {\bf 1}_{-1}^{T}  \bf{A}_0 $ and $ \bf{U}_1 =  {\bf 1}_{-1}^{T}  \bf{A}_1 $.
# 
# This means that $ {\bf u}_t $ is a Gaussian random variable that we can write as follows:
# 
# $$
# {\bf u}_t = \bf{U}_0 + \bf{U}_1 {\bf y}_t + \varepsilon_t^{(u)}  = \bf{U}_0 + \bf{U}_1^{(x)} {\bf x}_t + \bf{U}_1^{(z)} {\bf z}_t + \varepsilon_t^{(u)} 
# $$
# 
# where $ \varepsilon_t^{(u)} \sim \mathcal{N}(0,\Sigma_u) $ is a Gaussian random noise.  
# 
# The most important feature of this expression that we need going forward is is linear dependence on the state $ {\bf x}_t $. 
# This is the only result that we will use in order to construct a simple dynamic market model resulting from our IRL model. We use a deterministic limit of this equation, where in addition we set $ \bf{U}_0 = \bf{U}_1^{(z)} = 0 $, and replace $ \bf{U}_1^{(x)} \rightarrow \phi $ to simplify the notation. We thus obtain a simple deterministic policy
# 
# $$
# \label{determ_u}
# {\bf u}_t =  \phi  {\bf x}_t 
# $$
# 
# Next, let us recall the state equation and return equation (where we reinstate a time step $ \Delta t $,
# and $ \circ $ stands for an element-wise (Hadamard) product):
# 
# $$
# X_{t+ \Delta t} = (1 + r_t \Delta t) \circ (  X_t +  u_t  \Delta t)  
# $$
# $$
# r_t   = r_f + {\bf w} {\bf z}_t -  \mu  u_t + \frac{\sigma}{ \sqrt{ \Delta t}} \varepsilon_t 
# $$
# where $ r_f $ is a risk-free rate, $ \Delta t $ is a time step, $ {\bf z}_t $ is a vector of predictors with weights $ {\bf w} $, $ \mu $ is a market impact parameter with a linear impact specification, and $ \varepsilon_t \sim \mathcal{N} (\cdot| 0, 1) $ is a white noise residual.
# 
# 
# Eliminating $ u_t $ from these expressions and simplifying, we obtain
# $$ \Delta  X_t = \mu  \phi  ( 1 + \phi \Delta t) \circ  X_t \circ \left(  \frac{r_f (1 + \phi \Delta t)  + \phi}{ \mu \phi (1+ \phi \Delta t )}  -  X_t \right) \Delta t + 
# ( 1 + \phi \Delta t) X_t  \circ \left[ {\bf w} {\bf z}_t  \Delta t +  \sigma \sqrt{ \Delta t} \varepsilon_t \right]
# $$
# Finally, assuming that $ \phi \Delta t \ll 1 $ and taking the continuous-time limit $  \Delta t \rightarrow dt $, we obtain 
# 
# $$
# d X_t = \kappa \circ X_t \circ \left( \frac{\theta}{\kappa} - X_t \right) dt +  X_t \circ \left[ {\bf w} {\bf z}_t \, dt + \sigma d W_t \right]
# $$
# where $\kappa   =   \mu  \phi $, $ \theta  =   r_f + \phi $, and $ W_t $ is a standard Brownian motion.
# 
# Please note that this equation describes dynamics with a quadratic mean reversion. It is quite different from models with linear mean reversion such as the Ornstein-Uhlenbeck (OU) process. 
# 
# Without signals $ {\bf z}_t $, this process is known in the literature as a Geometric Mean Reversion (GMR) process. It has been used (for a one-dimensional setting) by Dixit and Pyndick (" Investment Under Uncertainty", Princeton 1994), and investigated (also for 1D) by Ewald and Yang ("Geometric Mean Reversion: Formulas for the Equilibrium Density and Analytic Moment Matching", {\it University of 
# St. Andrews Economics Preprints}, 2007). We have found that such dynamics (in a multi-variate setting) can also be obtained for market caps (or equivalently for stock prices, so long as the number of shares is held fixed) using Inverse Reinforcement Learning! 
# 
# (For more details, see I. Halperin and I. Feldshteyn, "Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy.
# (or, How We Learned to Stop Worrying and Love Bounded Rationality)", https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3174498) 
# 

# In[25]:


import pandas as pd
import numpy as np
import tensorflow as tf

import matplotlib.pyplot as plt
from datetime import datetime


# In[26]:


# read the data to a Dataframe
df_cap = pd.read_csv('dja_cap.csv')


# In[27]:


# add dates
dates = pd.bdate_range(start='2010-01-04', end=None, periods=df_cap.shape[0], freq='B')
df_cap['date'] = dates

df_cap.set_index('date',inplace=True)
df_cap.head()


# ## Let us build some signals 
# 
# Here we provide a "warm start" by computing two simple moving average signals that you can use as benchmark in your analysis.

# ### Generate moving averages

# In[28]:


# Calculating the short-window (10 days) simple moving average

window_1 = 10

short_rolling = df_cap.rolling(window=window_1).mean()
# short_rolling.head(20)


# In[29]:


# Calculating the long-window (30 days) simple moving average

window_2 = 30
long_rolling = df_cap.rolling(window=window_2).mean()
# long_rolling.tail()


# ### Plot three years of AAPL stock:

# In[30]:


ticker = 'AAPL'
start_date = '2015-01-01'
end_date = '2017-12-31'

fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)

ax.plot(df_cap.loc[start_date:end_date, :].index, df_cap.loc[start_date:end_date, 'AAPL'], label='Cap')
ax.plot(long_rolling.loc[start_date:end_date, :].index, long_rolling.loc[start_date:end_date, 'AAPL'], 
          label = '%d-days SMA' % window_2)
ax.plot(short_rolling.loc[start_date:end_date, :].index, short_rolling.loc[start_date:end_date, 'AAPL'], 
         label = '%d-days SMA' % window_1)

ax.legend(loc='best')
ax.set_ylabel('Cap in $')

plt.show()


# ## Part 1: Model calibration with moving average signals (Max 10 points)

# Recall the equation for the dynamics of market portfolio: 
# 
# $$ \Delta {\bf x}_t = \kappa_x \circ  {\bf x}_t \circ 
# \left( {\bf W}{\bf z}_t'  - {\bf x}_t \right)  +  {\bf x}_t  \circ \varepsilon_t^{(x)} $$
# 
# Here we change the notation a bit. Now $ {\bf z}_t' $ is an extended vector of predictors that includes a constant unit predictor $  {\bf z}_t' = [1, {\bf z}_t ]^T $. Therefore, for each name, if you have $ K = 2 $ signals, an extended vector of signals $ {\bf z}_t' $ is of length $ K + 1 $, and the  $ W $ stands for a factor loading matrix.
# The negative log-likelihood function for observable data with this model is therefore
# 
# $$  LL_M (\Theta) = - \log \prod_{t=0}^{T-1} 
# \frac{1}{ \sqrt{ (2 \pi)^{N}  \left| \Sigma_x \right| }} 
# e^{ - \frac{1}{2} \left(   {\bf v}_t
#  \right)^{T} 
# \Sigma_x^{-1}  
# \left(  {\bf v}_t \right)} $$
# 
# where
# 
# $$  {\bf v}_t \equiv \frac{{\bf x}_{t+1} -  {\bf x}_{t}}{{\bf x}_{t}}  
# -  \kappa_x \circ \left({\bf W} {\bf z}_t'   - {\bf x}_t \right)  $$
# 
# and $ \Sigma_x $ is the covariance matrix that was specified above in terms of other parameters. Here we directly infer the value of $ \Sigma_x $, along with other parameters, from data, so we will not use these previous expressions. 
# 
# Parameters that you have to estimate from data are therefore the vector of mean reversion speed 
# parameters $ \kappa_x $, factor loading matrix $ {\bf W} \equiv {\bf w}_z' $, and covariance matrix $ \Sigma_x $. 
# 
# Now, you are free to impose some structure on this parameters. Here are some choice, in the order of increasing complexity:
# 
# - assume that all values in vector-valued and matrix-valued parameters are the same, so that they can parametrized by scalars, e.g. $ \kappa_x = \kappa {\bf 1}_N $ where $ \kappa $ is a scalar value, and $ {\bf 1}_N $ is a vector of ones of length $ N $ where $ N $ is the number of stocks in the market portfolio. You can proceed similarly with specification of factor loading matrix $ W' $. Assume that all values in (diagonal!) factor loading matrices are the same for all names, and assume that all correlations and variances in the covariance matrix $ \Sigma_x $ are the same for all names.   
# 
# - Assume that all values are the same only within a given industrial sector.
# 
# 
# - You can also change the units. For example, you can consider logs of market caps instead of market caps themselves, ie. change the variable from $ {\bf x}_t  $ to $ {\bf q}_t = \log {\bf x}_t $
# 

# In[31]:


# Put the rest of you code and analysis for Part I here 

# Number of stocks in the market portfolio
T = df_cap.shape[0]
N = df_cap.shape[1]

# Select the available (not NaN) data from 'short_rolling' and 'long_rolling'
t0_short = int(np.where(short_rolling.index == short_rolling.first_valid_index())[0])
t0_long = int(np.where(long_rolling.index == long_rolling.first_valid_index())[0])
short_rolling_nna = short_rolling[t0_short:(T - 1)]
long_rolling_nna = long_rolling[t0_long:(T - 1)]

# Construct the time series for the market cap
mkt_cap = df_cap.sum(axis=1)
avg_mkt_cap = mkt_cap.mean()

# Scale the data
# Determine the long and short average rolling values and demean
short_rolling_avg_nna = short_rolling_nna / avg_mkt_cap
long_rolling_avg_nna = long_rolling_nna / avg_mkt_cap

# The data
signal_1 = short_rolling_avg_nna
signal_2 = long_rolling_avg_nna


# In[32]:


# Plot the index and rolling averages
fig=plt.figure(figsize=(14, N))
plt.suptitle('Market cap vs fitted mean reversion level (returns in Billion USD)',size=20)
plt.subplots_adjust(top=.95)

stocks = df_cap.columns[:N]
for index, stock in enumerate(stocks,1):
    plt.subplot(np.ceil(N/3),3,index)
    plt.plot(df_cap.index, df_cap[[stock]].values*1e-9, label='Market Index')
    plt.plot(short_rolling_nna.index, short_rolling_nna[[stock]].values*1e-9, label='10 days rolling')
    plt.plot(long_rolling_nna.index, long_rolling_nna[[stock]].values*1e-9, label='30 days rolling')
    plt.title(stock,size=12)
    plt.xticks([])             


# In[33]:


plt.show()


# In[34]:


from sklearn.preprocessing import StandardScaler
# Consider log returns instead. Demean and scale to unit variance

log_df_cap_mat_std = StandardScaler().fit_transform(df_cap.values)
log_df_cap_std = pd.DataFrame(data = log_df_cap_mat_std, 
                              index = df_cap.index,
                              columns = df_cap.columns.values)
log_short_rolling = log_df_cap_std.rolling(window=window_1).mean()
log_long_rolling = log_df_cap_std.rolling(window=window_2).mean()

fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)
plt.suptitle('Market cap demeaned and stanadrdized for AAPL',size=20)

ax.plot(log_df_cap_std.index, log_df_cap_std.loc[:, 'AAPL'], label='Log-Returns std')
ax.plot(log_long_rolling.index, log_long_rolling.loc[:, 'AAPL'], 
          label = '%d-days SMA' % window_2)
ax.plot(log_short_rolling.index, log_short_rolling.loc[:, 'AAPL'], 
         label = '%d-days SMA' % window_1)

ax.legend(loc='best')
ax.set_ylabel('log-ret')

plt.show()

# From the graph we can see that there is a clear upward trend


# ## Part 2: Propose and analyse your own signals  (Max 10 points)
# 
# In this part, you will experiment with other signals. Propose a signal and explain why it is interesting to 
# include this signal in the portfolio analysis. Then add your favorite signal or signals to the previous benchmarck signals (or alternatively you can replace them), and repeat the analysis of model calibration. State your conclusions.
# 

# In[35]:


# Put the rest of your code and analysis for Part 2 here.
from sklearn.preprocessing import MinMaxScaler

# Converting data to a log space and scale to distributions
df_cap_log = np.log(df_cap)
short_rolling_log = df_cap_log.rolling(window=window_1).mean()
long_rolling_log = df_cap_log.rolling(window=window_2).mean()

df_cap_scaler = MinMaxScaler([0.1, 1]).fit(df_cap_log)

df_cap_norm = pd.DataFrame(data=df_cap_scaler.transform(df_cap_log),
                           columns=df_cap_log.columns,
                           index=df_cap_log.index)

# Rescaling the signals
long_rolling_norm = pd.DataFrame(data=df_cap_scaler.transform(long_rolling_log.dropna()),
                                 columns=long_rolling_log.columns,
                                 index=long_rolling_log.dropna().index)

short_rolling_norm = pd.DataFrame(data=df_cap_scaler.transform(short_rolling_log.dropna()),
                                  columns=short_rolling_log.columns,
                                  index=short_rolling_log.dropna().index)

# Signals ar ethe percentage changes
signal_1 = short_rolling_norm.pct_change(periods=1).shift(-1).dropna().copy()
signal_2 = long_rolling_norm.pct_change(periods=1).shift(-1).dropna().copy()

# Consider the same times
market = df_cap_norm[df_cap_norm.index.isin(signal_1.index) & df_cap_norm.index.isin(signal_2.index)]
signal_1 = signal_1[signal_1.index.isin(market.index)]
signal_2 = signal_2[signal_2.index.isin(market.index)]


# In[36]:


# Inverse Reinforcement Learning code 
def fit_IRL_model(num_timesteps, signal_1, signal_2, epochs=1000, tol=1, learning_rate=1.5e-3):

    start = time.time()
    assert not np.any(np.isnan(market))
    assert not np.any(np.isnan(signal_1))
    assert not np.any(np.isnan(signal_2))

    t = market.shape[0]
    n = market.shape[1]

    # Set up the graph
    tf.reset_default_graph()
    x  = tf.placeholder(shape = (None, n), dtype = tf.float32, name = 'x' )  # market
    z1 = tf.placeholder(shape = (None, n), dtype = tf.float32, name = 'z1' ) # signal 1
    z2 = tf.placeholder(shape = (None, n), dtype = tf.float32, name = 'z2' ) # sigmal 2

    # Weights
    w1_init = tf.random_normal([n], mean = 0.5, stddev = 0.1)
    w2_init = 1 - w1_init
    w1      = tf.get_variable('w1', initializer = w1_init)
    w2      = tf.get_variable('w2', initializer = w2_init)
    W1      = w1 * tf.ones(n)
    W2      = w2 * tf.ones(n)

    # Parameters
    sigma = tf.get_variable('sigma', initializer = tf.random_uniform([n], minval=0.0, maxval=0.1))
    kappa = tf.get_variable('kappa', initializer = tf.random_uniform([n], minval=0.0, maxval=1.0))
    g     = tf.get_variable('g',     initializer = tf.random_uniform([n], minval=0.0, maxval=1.0))
    theta = tf.get_variable('theta', initializer = tf.random_uniform([n], minval=0.0, maxval=1.0))
    mu    = tf.zeros([n])
    Sigma = sigma * tf.ones([n])
    Kappa = kappa * tf.ones([n])
    G     = g     * tf.ones([n])
    Theta = theta * tf.ones([n])
    signals1 = tf.multiply(W1, z1)
    signals2 = tf.multiply(W2, z2)
    scale    = tf.slice(x, [0,0], [1,-1])
    signals  = tf.multiply(scale, tf.cumprod(1 + tf.add(signals1, signals2)))

    # Non-negative constraints for the weights
    w1_abs      = w1.assign(tf.maximum(0., w1))
    w2_abs      = w2.assign(tf.maximum(0., w2))
    g_abs       = g.assign(tf.maximum(0., g))
    constraints = tf.group(w1_abs, w2_abs)
                
    # IRL expression
    term0 = tf.add(tf.subtract(Theta, 0.5 * tf.square(Sigma)), signals)
    term1 = tf.multiply(-term0, x)
    term2 = tf.multiply(Kappa, tf.exp(x))
    term3 = tf.multiply(0.5 * G, tf.exp(2 * x))
    v     = tf.add(tf.add(term1, term2), term3)
    V = tf.slice(v, [0,0], [tf.shape(v)[0]-1, -1])

    distribution = tf.contrib.distributions.MultivariateNormalDiag(loc=mu, scale_diag=Sigma)
    log_prob     = distribution.log_prob(V)
    reg_term     = tf.reduce_sum(tf.square(w1 + w2 - 1))
    neg_log_likelihood = 0.01 * reg_term - tf.reduce_sum(log_prob)
    
    # Configuration the optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)
    train_op  = optimizer.minimize(neg_log_likelihood)
        
    print('The optimizer has been configured!')
    
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        # run the first iteration and calculate initial loss
        res = []
        loss = []
        res.append(sess.run(neg_log_likelihood, feed_dict = {x: market, z1: signal_1, z2: signal_2}))

        # Iterate loss minimization in a loop
        e=1
        while True:
            # Solve the optimiser
            sess.run(train_op, feed_dict={x: market, z1: signal_1, z2: signal_2})
            sess.run(constraints) 

            # Update loss
            res.append(sess.run(neg_log_likelihood, feed_dict={x: market, z1: signal_1, z2: signal_2}))
            loss_ = np.abs(res[-1] - res[-2])
            loss.append(loss_)

            if (e % 200 == 0) or (e == 1):
                print('Epoch {:5}: Loss: {}'.format(e, loss_))

            if loss_ < tol:
                print('Converged after {} epochs. Loss: {}'.format(e, loss_))
                break

            if e >= epochs:
                print('Specified maximum epochs ({}) reached without convergence. Loss: {}'.format(epochs, loss_))
                break

            e += 1
                
        end = time.time()            
            
        # Save the coefficients
        coefficients = pd.DataFrame([], index = market.columns, columns = ['kappa', 'sigma', 'theta', 'g', 'w1', 'w2'] )
        coefficients['kappa']   = sess.run(kappa)
        coefficients['sigma']   = sess.run(sigma)
        coefficients['theta']   = sess.run(theta)
        coefficients['g']       = sess.run(g)
        coefficients['w1']      = sess.run(W1)
        coefficients['w2']      = sess.run(W2)

        est_ = sess.run(signals, feed_dict={x: market, z1: signal_1, z2: signal_2})
        estimation = pd.DataFrame(est_, index=market.index, columns=market.columns)
    
    return estimation
    print('All done. Elapsed time:', timedelta(seconds=end-start))


# In[40]:


import time, random
from datetime import timedelta


tf.logging.set_verbosity(tf.logging.ERROR)
estimation_scaled = fit_IRL_model(market, signal_1, signal_2, epochs=5000, tol=2)


# In[41]:


estimation = df_cap_scaler.inverse_transform(estimation_scaled)
df_est = pd.DataFrame(estimation, columns=estimation_scaled.columns, index=estimation_scaled.index)


# In[42]:



fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)
plt.suptitle('Market cap demeaned and stanadrdized for AAPL',size=20)

ax.plot(df_cap_log.index, df_cap_log.loc[:, 'AAPL'], label='Original')
ax.plot(df_est.index, df_est.loc[:, 'AAPL'], 
          label = 'Estimator')

ax.legend(loc='best')
ax.set_ylabel('log-ret')

plt.show()


# ## Part 3: Can you do it for the S&P500 market portfolio? (Max 10 point)
# 
# Try to repeat your analysis for the S&P500 portfolio. 
# 
# The data can be obtained from Course 2 "Fundamentals of Machine Learning in Finance" in this Specialization.

# In[16]:


# Put the rest of your code and analysis for Part 3 here.
import os
cwd = os.getcwd()
#print(cwd)
os.listdir('/home/jovyan/work/readonly')

# load dataset
asset_prices = pd.read_csv('/home/jovyan/work/readonly/spx_holdings_and_spx_closeprice.csv',
                           date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),
                           index_col = 0).dropna()
n_stocks_show = 12
print('Asset prices shape', asset_prices.shape)
asset_prices.iloc[:, :n_stocks_show].head()


# In[17]:


spx_df_cap_log = np.log(asset_prices)
spx_short_rolling_log = spx_df_cap_log.rolling(window=window_1).mean()
spx_long_rolling_log = spx_df_cap_log.rolling(window=window_2).mean()

spx_df_cap_scaler = MinMaxScaler([0.1, 1]).fit(dji_df_cap_log)

spx_df_cap_norm = pd.DataFrame(data=spx_df_cap_scaler.transform(spx_df_cap_log),
                               columns=spx_df_cap_log.columns,
                               index=spx_df_cap_log.index)

# Rescaling the signals
spx_long_rolling_norm = pd.DataFrame(data=spx_df_cap_scaler.transform(spx_long_rolling_log.dropna()),
                                     columns=spx_long_rolling_log.columns,
                                     index=spx_long_rolling_log.dropna().index)

spx_short_rolling_norm = pd.DataFrame(data=spx_df_cap_scaler.transform(spx_short_rolling_log.dropna()),
                                      columns=spx_short_rolling_log.columns,
                                      index=sspx_hort_rolling_log.dropna().index)

# Signals ar ethe percentage changes
spx_signal_1 = spx_short_rolling_norm.pct_change(periods=1).shift(-1).dropna().copy()
spx_signal_2 = spx_long_rolling_norm.pct_change(periods=1).shift(-1).dropna().copy()

# Consider the same times
spx_market = spx_df_cap_norm[spx_df_cap_norm.index.isin(spx_signal_1.index) &
                             spx_df_cap_norm.index.isin(spx_signal_2.index)]
spx_signal_1 = spx_signal_1[spx_signal_1.index.isin(spx_market.index)]
spx_signal_2 = spx_signal_2[spx_signal_2.index.isin(spx_market.index)]


# In[18]:


spx_estimation_scaled = fit_IRL_model(spx_market, spx_signal_1, spx_signal_2, epochs=5000, tol=1e-3)


# In[19]:


spx_estimation = spx_df_cap_scaler.inverse_transform(spx_estimation_scaled)
spx_df_est = pd.DataFrame(spx_estimation, columns=spx_estimation_scaled.columns, index=spx_estimation_scaled.index)


# In[ ]:


fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(1,1,1)
plt.suptitle('SPX Market cap demeaned and standardized for AAPL',size=20)

ax.plot(spx_df_cap_log.index, spx_df_cap_log.loc[:, 'AAPL'], label='Original')
ax.plot(spx_df_est.index, spx_df_est.loc[:, 'AAPL'], 
          label = 'Estimator')

ax.legend(loc='best')
ax.set_ylabel('log-ret')

plt.show()


# ## Part 4 (Optional): Show me something else.
# 
# Here you can develop any additional analysis of the model that you may find interesting (One possible suggestion is 
# presented above, but you should feel free to choose your own topic). Present your case and finding/conclusions.
# 

# In[10]:


# Put the rest of your code and analysis for Part 3 here.


# In[ ]:




